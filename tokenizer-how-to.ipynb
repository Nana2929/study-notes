{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Tokenization\n",
    "Author: Ching Wen Yang\n",
    "\n",
    "- In this notebook, we particularly focus on **tokenization**, which is the first step of the compilation process. \n",
    "<img src=\"https://i.imgur.com/CbNSzT4.png\" alt=\"drawing\" width=\"400\"/>\n",
    "- `tokenizer`, also known as a `lexer` \n",
    "- **IMPORTANT:** Tokenization fundamentally works on a stream of characters. The input does not need to be a valid python string; it can be a potential beginning of a python string.\n",
    "\n",
    "## References\n",
    "- [Brown Water Python Documentation](https://www.asmeurer.com/brown-water-python/intro.html)\n",
    "- [Nana's Compiler Study Notes (HackMD)](https://hackmd.io/nFUOy3eoQYyRLQh0VvSdRw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=62 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')\n",
      "string split: utf-8\n",
      "TokenInfo(type=1 (NAME), string='print', start=(1, 0), end=(1, 5), line=\"print('Hello World')\\n\")\n",
      "string split: print\n",
      "TokenInfo(type=54 (OP), string='(', start=(1, 5), end=(1, 6), line=\"print('Hello World')\\n\")\n",
      "string split: (\n",
      "TokenInfo(type=3 (STRING), string=\"'Hello World'\", start=(1, 6), end=(1, 19), line=\"print('Hello World')\\n\")\n",
      "string split: 'Hello World'\n",
      "TokenInfo(type=54 (OP), string=')', start=(1, 19), end=(1, 20), line=\"print('Hello World')\\n\")\n",
      "string split: )\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 20), end=(1, 21), line=\"print('Hello World')\\n\")\n",
      "string split: \n",
      "\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')\n",
      "string split: \n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "import io \n",
    "string = \"print('Hello World')\\n\"\n",
    "g = tokenize.tokenize(io.BytesIO(string.encode('utf-8')).readline)\n",
    "for token in g:\n",
    "    print(token)\n",
    "    print('string split:', token.string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A failed example\n",
    "- Omitting closing parenthesis\n",
    "- The input need not be semantically meaningful in any way. The input string, even if completed, can only raise a TypeError because `\"a\" + True` is not allowed by Python. **The tokenize module does not know or care about objects, types, or any high-level Python constructs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _tokenize at 0x7f6e8f4572e0>\n",
      "TokenInfo(type=62 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')\n",
      "TokenInfo(type=54 (OP), string='(', start=(1, 0), end=(1, 1), line='(\"a\" + True -')\n",
      "TokenInfo(type=3 (STRING), string='\"a\"', start=(1, 1), end=(1, 4), line='(\"a\" + True -')\n",
      "TokenInfo(type=54 (OP), string='+', start=(1, 5), end=(1, 6), line='(\"a\" + True -')\n",
      "TokenInfo(type=1 (NAME), string='True', start=(1, 7), end=(1, 11), line='(\"a\" + True -')\n",
      "TokenInfo(type=54 (OP), string='-', start=(1, 12), end=(1, 13), line='(\"a\" + True -')\n",
      "('EOF in multi-line statement', (2, 0))\n"
     ]
    }
   ],
   "source": [
    "from tokenize import TokenError\n",
    "string = '(\"a\" + True -'\n",
    "g = tokenize.tokenize(io.BytesIO(string.encode('utf-8')).readline)  \n",
    "print(g)\n",
    "while True:\n",
    "    try:\n",
    "        token = next(g)\n",
    "        print(token)\n",
    "    except TokenError as e:\n",
    "        print(e) \n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tokenize` vs. Alternatives\n",
    "- When one wants to find or modify syntactic constructs in Python source code, we can do \n",
    "    - lexer, eg. `tokenize` module \n",
    "    - ast module, eg. `ast` module\n",
    "    - `re` (regular expression): Very hard to detect syntax correctly, hence skipped\n",
    "\n",
    "### Use tokenizer to get the start and end line numbers for a function\n",
    "We see that it easily gets the start and end line numbers for a function. \n",
    "Also, it recognizes the string as a string and does not mistokenize the function name inside as a keyword.\n",
    "![image.png](images/syntax_tool_comparison_table.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "import io\n",
    "def line_numbers_tokenize(inputcode):\n",
    "    for tok in tokenize.tokenize(io.BytesIO(inputcode.encode('utf-8')).readline):\n",
    "        if tok.type == tokenize.NAME and tok.string == 'def':\n",
    "            print(tok.start[0])\n",
    "\n",
    "code = \"\"\"\\\n",
    "def f(x):\n",
    "    pass\n",
    "\n",
    "class MyClass:\n",
    "    def g(self):\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "code_tricky = '''\\\n",
    "FUNCTION_SKELETON = \"\"\"\n",
    "def {name}({args}):\n",
    "    {body}\n",
    "\"\"\"\n",
    "'''\n",
    "line_numbers_tokenize(code)\n",
    "print(\"----------\")\n",
    "line_numbers_tokenize(code_tricky)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "### Calling Syntax \n",
    "`tokenize` API requires the `readline` method of a bytes-mode file-like object. \n",
    "`text` mode (`r`) is weirdly not supported.\n",
    "To tokenize a string, we can use `io.BytesIO` to wrap the string and pass it to `tokenize.tokenize`. \n",
    "\n",
    "### `TokenInfo` \n",
    "- `TokenInfo` is a named tuple with the following fields:\n",
    "    - `type`: The type of the token. This is one of the token constants listed below. \n",
    "    - `string`: The tokenâ€™s string representation (as in the source file). \n",
    "    - `start`: The starting (row, column) indices of the token. \n",
    "    - `end`: The ending (row, column) indices of the token. \n",
    "    - `line`: The line on which the token was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
