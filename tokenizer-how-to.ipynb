{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Tokenization\n",
    "Author: Ching Wen Yang\n",
    "\n",
    "- In this notebook, we particularly focus on **tokenization**, which is the first step of the compilation process. \n",
    "<img src=\"https://i.imgur.com/CbNSzT4.png\" alt=\"drawing\" width=\"400\"/>\n",
    "- `tokenizer`, also known as a `lexer` \n",
    "- **IMPORTANT:** Tokenization fundamentally works on a stream of characters. The input does not need to be a valid python string; it can be a potential beginning of a python string.\n",
    "\n",
    "## References\n",
    "- [Brown Water Python Documentation](https://www.asmeurer.com/brown-water-python/intro.html)\n",
    "- [Nana's Compiler Study Notes (HackMD)](https://hackmd.io/nFUOy3eoQYyRLQh0VvSdRw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=62 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')\n",
      "string split: utf-8\n",
      "TokenInfo(type=1 (NAME), string='print', start=(1, 0), end=(1, 5), line=\"print('Hello World')\\n\")\n",
      "string split: print\n",
      "TokenInfo(type=54 (OP), string='(', start=(1, 5), end=(1, 6), line=\"print('Hello World')\\n\")\n",
      "string split: (\n",
      "TokenInfo(type=3 (STRING), string=\"'Hello World'\", start=(1, 6), end=(1, 19), line=\"print('Hello World')\\n\")\n",
      "string split: 'Hello World'\n",
      "TokenInfo(type=54 (OP), string=')', start=(1, 19), end=(1, 20), line=\"print('Hello World')\\n\")\n",
      "string split: )\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 20), end=(1, 21), line=\"print('Hello World')\\n\")\n",
      "string split: \n",
      "\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')\n",
      "string split: \n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "import io \n",
    "string = \"print('Hello World')\\n\"\n",
    "g = tokenize.tokenize(io.BytesIO(string.encode('utf-8')).readline)\n",
    "for token in g:\n",
    "    print(token)\n",
    "    print('string split:', token.string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A failed example\n",
    "- Omitting closing parenthesis\n",
    "- The input need not be semantically meaningful in any way. The input string, even if completed, can only raise a TypeError because `\"a\" + True` is not allowed by Python. **The tokenize module does not know or care about objects, types, or any high-level Python constructs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _tokenize at 0x7fd6c9d89cf0>\n",
      "TokenInfo(type=62 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')\n",
      "TokenInfo(type=54 (OP), string='(', start=(1, 0), end=(1, 1), line='(\"a\" + True -')\n",
      "TokenInfo(type=3 (STRING), string='\"a\"', start=(1, 1), end=(1, 4), line='(\"a\" + True -')\n",
      "TokenInfo(type=54 (OP), string='+', start=(1, 5), end=(1, 6), line='(\"a\" + True -')\n",
      "TokenInfo(type=1 (NAME), string='True', start=(1, 7), end=(1, 11), line='(\"a\" + True -')\n",
      "TokenInfo(type=54 (OP), string='-', start=(1, 12), end=(1, 13), line='(\"a\" + True -')\n",
      "('EOF in multi-line statement', (2, 0))\n"
     ]
    }
   ],
   "source": [
    "from tokenize import TokenError\n",
    "string = '(\"a\" + True -'\n",
    "g = tokenize.tokenize(io.BytesIO(string.encode('utf-8')).readline)  \n",
    "print(g)\n",
    "while True:\n",
    "    try:\n",
    "        token = next(g)\n",
    "        print(token)\n",
    "    except TokenError as e:\n",
    "        print(e) \n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tokenize` vs. Alternatives\n",
    "- When one wants to find or modify syntactic constructs in Python source code, we can do \n",
    "    - lexer, eg. `tokenize` module \n",
    "    - ast module, eg. `ast` module\n",
    "    - `re` (regular expression): Very hard to detect syntax correctly, hence skipped\n",
    "\n",
    "### Use tokenizer to get the start and end line numbers for a function\n",
    "We see that it easily gets the start and end line numbers for a function. \n",
    "Also, it recognizes the string as a string and does not mistokenize the function name inside as a keyword.\n",
    "\n",
    "![image.png](images/syntax_tool_comparison_table.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "import io\n",
    "def line_numbers_tokenize(inputcode):\n",
    "    for tok in tokenize.tokenize(io.BytesIO(inputcode.encode('utf-8')).readline):\n",
    "        if tok.type == tokenize.NAME and tok.string == 'def':\n",
    "            print(tok.start[0])\n",
    "\n",
    "code = \"\"\"\\\n",
    "def f(x):\n",
    "    pass\n",
    "\n",
    "class MyClass:\n",
    "    def g(self):\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "code_tricky = '''\\\n",
    "FUNCTION_SKELETON = \"\"\"\n",
    "def {name}({args}):\n",
    "    {body}\n",
    "\"\"\"\n",
    "'''\n",
    "line_numbers_tokenize(code)\n",
    "print(\"----------\")\n",
    "line_numbers_tokenize(code_tricky)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "### Calling Syntax \n",
    "`tokenize` API requires the `readline` method of a bytes-mode file-like object. \n",
    "`text` mode (`r`) is weirdly not supported.\n",
    "To tokenize a string, we can use `io.BytesIO` to wrap the string and pass it to `tokenize.tokenize`. \n",
    "- `tokenize.generate_tokens()` can be used to generate tokens from a string. \n",
    "    - Do `tokenize.generate_tokens(io.BytesIO(string).readline)` to tokenize a string. \n",
    "    - Equivalent to `tokenize.tokenize(io.BytesIO(string).readline)` \n",
    "### `TokenInfo` \n",
    "- `TokenInfo` is a named tuple with the following fields:\n",
    "    - `type`: The type of the token. This is one of the token constants listed below. \n",
    "    - `string`: The tokenâ€™s string representation (as in the source file). \n",
    "    - `start`: The starting (row, column) indices of the token. \n",
    "    - `end`: The ending (row, column) indices of the token. \n",
    "    - `line`: The line on which the token was found."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=1 (NAME), string='def', start=(1, 0), end=(1, 3), line='def f(x):\\n')\n",
      "TokenInfo(type=1 (NAME), string='f', start=(1, 4), end=(1, 5), line='def f(x):\\n')\n",
      "TokenInfo(type=54 (OP), string='(', start=(1, 5), end=(1, 6), line='def f(x):\\n')\n",
      "TokenInfo(type=1 (NAME), string='x', start=(1, 6), end=(1, 7), line='def f(x):\\n')\n",
      "TokenInfo(type=54 (OP), string=')', start=(1, 7), end=(1, 8), line='def f(x):\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(1, 8), end=(1, 9), line='def f(x):\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 9), end=(1, 10), line='def f(x):\\n')\n",
      "TokenInfo(type=5 (INDENT), string='    ', start=(2, 0), end=(2, 4), line='    pass\\n')\n",
      "TokenInfo(type=1 (NAME), string='pass', start=(2, 4), end=(2, 8), line='    pass\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(2, 8), end=(2, 9), line='    pass\\n')\n",
      "TokenInfo(type=61 (NL), string='\\n', start=(3, 0), end=(3, 1), line='\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(4, 0), end=(4, 0), line='class MyClass:\\n')\n",
      "TokenInfo(type=1 (NAME), string='class', start=(4, 0), end=(4, 5), line='class MyClass:\\n')\n",
      "TokenInfo(type=1 (NAME), string='MyClass', start=(4, 6), end=(4, 13), line='class MyClass:\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(4, 13), end=(4, 14), line='class MyClass:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(4, 14), end=(4, 15), line='class MyClass:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='    ', start=(5, 0), end=(5, 4), line='    def g(self):\\n')\n",
      "TokenInfo(type=1 (NAME), string='def', start=(5, 4), end=(5, 7), line='    def g(self):\\n')\n",
      "TokenInfo(type=1 (NAME), string='g', start=(5, 8), end=(5, 9), line='    def g(self):\\n')\n",
      "TokenInfo(type=54 (OP), string='(', start=(5, 9), end=(5, 10), line='    def g(self):\\n')\n",
      "TokenInfo(type=1 (NAME), string='self', start=(5, 10), end=(5, 14), line='    def g(self):\\n')\n",
      "TokenInfo(type=54 (OP), string=')', start=(5, 14), end=(5, 15), line='    def g(self):\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(5, 15), end=(5, 16), line='    def g(self):\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(5, 16), end=(5, 17), line='    def g(self):\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(6, 0), end=(6, 8), line='        pass\\n')\n",
      "TokenInfo(type=1 (NAME), string='pass', start=(6, 8), end=(6, 12), line='        pass\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(6, 12), end=(6, 13), line='        pass\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(7, 0), end=(7, 0), line='')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(7, 0), end=(7, 0), line='')\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(7, 0), end=(7, 0), line='')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'sum([[1, 2]][0])'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate_tokens() \n",
    "for t in tokenize.generate_tokens(io.StringIO(code).readline):\n",
    "    print(t) \n",
    "# untokenize()\n",
    "string = b'sum([[1, 2]][0])'\n",
    "tokenize.untokenize(tokenize.tokenize(io.BytesIO(string).readline))\n",
    "b'sum([[1, 2]][0])'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptions \n",
    "- `SyntaxError` is raised when the input has an invalid encoding. \n",
    "    - `detect_encoding` detects using SheBang line or a Unicode BOM. \n",
    "    - This is where `tokenize()` differs from `generate_tokens()`; the latter ignores the encoding line. \n",
    "- `TokenError` is raised when the input contains an invalid token. \n",
    "    - 2 scenarios \n",
    "        - EOF in multi-line string. (Wrong docstring format)\n",
    "        - EOF in multi-line statement. (Unclosed braces)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Types\n",
    "- `tokenize.tok_name` dictionary \n",
    "### The Tokens \n",
    "- `ENDMARKER`: end of line unless EOF error \n",
    "- `NAME`: identifiers and keywords, to tell them apart use `keyword.iskeyword()`\n",
    "- `NUMBER`: integers, floats, imaginary (including binary, octal literals and fps etc.)\n",
    "   - Will retain the original input format using the below `print_tokens()`\n",
    "   - But if use `ast` to do `ast.dump(ast.parse(\"1.0000001\"))`, it will be rounded.\n",
    "   - Note examples such as `123_456`; in Python 3.6 above or below they are tokenized differently. See [link](https://www.asmeurer.com/brown-water-python/tokens.html#number). \n",
    "- `STRING`: single and double quotes, triple quotes (docstrings), f-strings, bytes, raw strings, unicode strings, etc. \n",
    "- `NEWLINE`: Represents `\\n` or `\\r\\n` that ends a logical line of Python code. `NEWLINE` that does not end a logical line of Python code uses the `NL` token type.\n",
    "- `INDENT`: Represents an increase in indentation level. The indentation itself is a `string`. \n",
    "   - indentation can be any number of spaces or tabs. \n",
    "   - Every unindented level must match a previous outer indentation level, otherwise an `IndentationError` is raised.\n",
    "- `RARROW` and `ELLIPSIS`\n",
    "   - tokenize as `OP`. \n",
    "- `OP`\n",
    "   - a generic token type for all operators, delimiters and the ellpsis literals. \n",
    "   - If operators are not recognized by the parser, they are `ERRORTOKEN` instead. \n",
    "   - `LPAR`, `RPAR`, `LSQB`, `RSQB`, `COLON`, `COMMA`, `SEMI`, `PLUS`, `MINUS`, `STAR`, `SLASH`, `VBAR`, `AMPER`, `LESS`, `GREATER`, `EQUAL`, `DOT`, `PERCENT`, `LBRACE`, `RBRACE`, `EQEQUAL`, `NOTEQUAL`, `LESSEQUAL`, `GREATEREQUAL`, `TILDE`, `CIRCUMFLEX`, `LEFTSHIFT`, `RIGHTSHIFT`, `DOUBLESTAR`, `PLUSEQUAL`, `MINEQUAL`, `STAREQUAL`, `SLASHEQUAL`, `PERCENTEQUAL`, `AMPEREQUAL`, `VBAREQUAL`, `CIRCUMFLEXEQUAL`, `LEFTSHIFTEQUAL`, `RIGHTSHIFTEQUAL`, `DOUBLESTAREQUAL`, `DOUBLESLASH`, `DOUBLESLASHEQUAL`, `AT`, `ATEQUAL`, `RARROW`, `ELLIPSIS`, `COLONEQUAL`, `OP` \n",
    "- `AWAIT`, `ASYNC` \n",
    "   - `AWAIT` and `ASYNC` are only recognized in Python 3.5 and 3.6. \n",
    "   - psuedo keywords: to aid the transition in the addition of new keywords. \n",
    "   - Both are kept as valid variable names if OUTSIDE an `async def` function. \n",
    "   - In Python 3.7, `async` and `await` are promoted to proper keywords, and have been removed from the `tokenize` module. \n",
    "   - Make sure you set the `feature_version` flag for `ast.parse` to parse them correctly. \n",
    "- `TYPE_IGNORE`, `TYPE_COMMENT`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(s):\n",
    "    for t in tokenize.tokenize(io.BytesIO(s.encode('utf-8')).readline):\n",
    "        print(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=62 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')\n",
      "TokenInfo(type=2 (NUMBER), string='1.0', start=(1, 0), end=(1, 3), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=54 (OP), string='+', start=(1, 3), end=(1, 4), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='0b101', start=(1, 4), end=(1, 9), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=54 (OP), string='+', start=(1, 10), end=(1, 11), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='0o10', start=(1, 12), end=(1, 16), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=54 (OP), string='+', start=(1, 17), end=(1, 18), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='0xa', start=(1, 19), end=(1, 22), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=54 (OP), string='+', start=(1, 23), end=(1, 24), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1e1', start=(1, 25), end=(1, 28), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=54 (OP), string='+', start=(1, 29), end=(1, 30), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='100j', start=(1, 31), end=(1, 35), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 35), end=(1, 36), line='1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n')\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "number_str = '1.0+0b101 + 0o10 + 0xa + 1e1 + 100j\\n'\n",
    "print_tokens(number_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=62 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')\n",
      "TokenInfo(type=61 (NL), string='\\n', start=(1, 0), end=(1, 1), line='\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1', start=(2, 0), end=(2, 1), line='1\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(2, 1), end=(2, 2), line='1\\n')\n",
      "TokenInfo(type=5 (INDENT), string='    ', start=(3, 0), end=(3, 4), line='    2\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='2', start=(3, 4), end=(3, 5), line='    2\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(3, 5), end=(3, 6), line='    2\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='3', start=(4, 4), end=(4, 5), line='    3\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(4, 5), end=(4, 6), line='    3\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(5, 0), end=(5, 8), line='        4\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='4', start=(5, 8), end=(5, 9), line='        4\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(5, 9), end=(5, 10), line='        4\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(6, 0), end=(6, 0), line='5\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(6, 0), end=(6, 0), line='5\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='5', start=(6, 0), end=(6, 1), line='5\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(6, 1), end=(6, 2), line='5\\n')\n",
      "TokenInfo(type=61 (NL), string='\\n', start=(7, 0), end=(7, 1), line='\\n')\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(8, 0), end=(8, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "# INDENT \n",
    "print_tokens(\"\"\"\n",
    "1\n",
    "    2\n",
    "    3\n",
    "        4\n",
    "5\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=62 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')\n",
      "TokenInfo(type=1 (NAME), string='X', start=(1, 0), end=(1, 1), line='X @ Y')\n",
      "TokenInfo(type=54 (OP), string='@', start=(1, 2), end=(1, 3), line='X @ Y')\n",
      "TokenInfo(type=1 (NAME), string='Y', start=(1, 4), end=(1, 5), line='X @ Y')\n",
      "TokenInfo(type=4 (NEWLINE), string='', start=(1, 5), end=(1, 6), line='')\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "string = \"X @ Y\"\n",
    "print_tokens(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
